#+TITLE: Best Practices
#+AUTHOR: Lalit Kumar
#+EMAIL: lalitkumar.meena.lk@gmail.com
#+OPTIONS: toc:nil

* Loops
  Yes, loops are faster. map/reduce/filter can have method call overhead (sometimes not, if the function gets inlined by the JIT engine), and have a bunch of other overhead to handle obscure corner cases like sparse arrays and getters.
  There are even libraries that reimplement map/filter/reduce for the sole purpose of providing faster drop-in alternatives to those methods.
  However, beware that loops that declare variables using var share context between iterations, which can be a source of bugs if creating closures within the loop
#+begin_src javascript
  for (var i = 0; i < elements.length; i++) {
    elements[i].onclick = function() {
      alert(i); // bug: i is always elements.length
    };
  }
#+end_src
  Another thing that is worth mentioning: there are old articles around the web benchmarking several convoluted ways of writing loops (caching length, reverse while loops, etc). These days, performance-wise, those techniques are obsolete, because JIT engines are now smart enough to correctly optimize idiomatic loops. If you're using loops, always use them idiomatically.
  Anyone that tells you to prefer map/reduce/filter is coming from the point of view of maintainability: map/reduce/filter are generally less "noisy" (read: they don't explicitly require a i++), though frankly, both map/reduce/filter loops and for loops are quite readable and maintainable (see e.g. lodash source code).
  I'm going to against the grain here and say: don't waste time "profiling" for loops vs map/filter/reduce. For loops are faster. Period.
  With that being said, loop mechanics are likely the last thing you need to optimize. First you should look into algorithms to reduce the complexity of your operation (e.g. sometimes it's more efficient to use a hashmap for its fast lookup properties than an doing a linear scan of an array multiple times); second, seek to pull things out of loops (e.g. sometimes doing a step unconditionally to all items and undoing it once is faster than testing for the condition on every iteration, sometimes you can memoize parts of the loop body, etc); third, look for ways to reduce the cost of the loop body, e.g. break/continue early rather than late, simplify convoluted if/else branches, memoize, etc.
  When doing these other optimizations, then do profile. Unless you're writing a library that is highly sensitive to performance (e.g. a virtuam dom library), don't waste time optimizing if you don't actually perceive any benefits to doing so. Most important of all, if you're sacrificing idiomaticness for performance, alarms should be ringing in your head.
